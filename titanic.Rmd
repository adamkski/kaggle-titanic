---
title: 'kaggle tutorial: titanic survival'
author: "Adam Kowalczewski"
date: "27 July 2016"
output: html_document
---

```{r}
library(dplyr)
library(readr)
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(randomForest)
library(party)
setwd("~/R101/titanic/")

train <- read_csv("train.csv")
test <- read_csv("test.csv")
```

# prediction 1: everyone dies
```{r}
test$Survived <- rep(0, 418)
submit <- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)
write.csv(submit, file = "theyallperish.csv", row.names = F)
```

# prediction 2: women survive, men die

We can see that most women survived, while very few men did.  We can update our prediction to be that all women survive and see how our score changes.

```{r}
prop.table(table(as.factor(train$Sex), train$Survived),1)

test$Survived[test$Sex == 'female'] <- 1
submit <- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)
write.csv(submit, file = "womenfirst.csv", row.names = F)
```

# prediction 3: children survive, adults less so

We know the famous "Women and children first" approach to rescue the Titanic took so we can try using the age variable to get some more accuracy.

```{r}
summary(train$Age)

# assume the NAs were the average age (so will not be assigned child below)
train$Child <- 0
train$Child[train$Age < 18] <- 1

# see survivor rate for different subsets
# aggregate takes target variable left of ~ and subsets by vars on the right
aggregate(Survived ~ Child + Sex, data = train, FUN=sum)
# find totals for each subset
aggregate(Survived ~ Child + Sex, data = train, FUN=length)

# combine the two into a proportion
aggregate(Survived ~ Child + Sex, data = train, FUN=function(x){sum(x)/length(x)})



```

It looks like the gender dimension still explains most of the difference.  Being a child didn't rescue males all that much more... [or did it]?  So we won't make a submission here.

# prediction 4: higher socio economic status increased your chance of survival
We'll bin the continuous variable Fare so we can tabulate it.  We'll use less than $10, $10 - $20, $20 - $30, and more than $30.

It turns out that women in 3rd class who paid for a more expensive ticket were less likely to survive, perhaps because expensive cabins were futher from lifeboats?  We can update our prediction based on this.

```{r}
train$Fare2 <- '30+'
train$Fare2[train$Fare >= 20 & train$Fare < 30] <- '20 - 30'
train$Fare2[train$Fare >= 10 & train$Fare < 20] <- '10 - 20'
train$Fare2[train$Fare < 10] <- '<10'

aggregate(Survived ~ Fare2 + Pclass + Sex, data = train, FUN=function(x){sum(x) / length(x)})
```

```{r}
test$Survived <- 0
test$Survived[test$Sex == 'female'] <- 1
test$Survived[test$Sex == 'female' & test$Pclass == 3 & test$Fare >= 20] <- 0

submit <- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)
write.csv(submit, file = "women_class_combo_model.csv", row.names = F)

```

# prediction 5: using a decision tree

```{r}
str(train)
fit <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,
             data = train,
             method = "class")
# make an rpart plot
plot(fit)
text(fit)
# make a fancy rpart plot
fancyRpartPlot(fit)

# prepare a submission to kaggle
Prediction <- predict(fit, test, type = "class")
submit <- data.frame(PassengerID = test$PassengerId, Survived = Prediction)
write.csv(submit, file = "myfirsttree.csv", row.names = F)
```

# prediction 6: modifying controls on the decision tree

Lowering the complexity parameter threshold doesn't improve on prediction 5.
```{r}
fit <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,
             data = train,
             method = "class",
             control = rpart.control(cp = 0.005))
fancyRpartPlot(fit)

# prepare a submission to kaggle
Prediction <- predict(fit, test, type = "class")
submit <- data.frame(PassengerID = test$PassengerId, Survived = Prediction)
write.csv(submit, file = "mysecondtree.csv", row.names = F)
```

# prediction 7: feature engineering on titles

We'll try to extract some more information from some of the text fields.  We'll need to do the same thing to both training and testing datasets so we'll first bind them together.
```{r p7}
# since test dataset lacks Survived column, we'll add it
test$Survived <- NA
combi <- rbind(train, test)

# extract person's title from each row
combi$Title <- sapply(combi$Name, FUN = function(x) {strsplit(x, split = '[,.]')[[1]][2]})
# strip off leading spaces
combi$Title <- sub(' ', '', combi$Title)

# combine similar titles into single category
combi$Title[combi$Title %in% c('Mme', 'Mlle')] <- 'Mlle'
combi$Title[combi$Title %in% c('Capt', 'Don', 'Major', 'Sir')] <- 'Sir' # these are rich men
combi$Title[combi$Title %in% c('Dona', 'Lady', 'the Countess','Jonkheer')] <- 'Lady' # these are rich women
# change back to factor
combi$Title <- factor(combi$Title)

# large family sizes may also affect survival
combi$FamilySize <- combi$SibSp + combi$Parch + 1

# some families may have more trouble than others, create a family ID
combi$Surname <- sapply(combi$Name, FUN = function(x) {strsplit(x, split = '[,.]')[[1]][1]})
combi$FamilyID <- paste(as.character(combi$FamilySize), combi$Surname, sep = "")
# remove small families to avoid repition of family IDs (greater than 2 we assume no reps)
combi$FamilyID[combi$FamilySize <= 2] <- 'Small'
```

Some further cleanup is needed because a few families slip through our assumptions.  For instance, the Appleton family got a family size of 3 and yet seems to only have one member.

```{r}
famIDs <- data.frame(table(combi$FamilyID))
famIDs <- famIDs[famIDs$Freq <= 2,]
# overwrite the errors
combi$FamilyID[combi$FamilyID %in% famIDs$Var1] <- 'Small'
combi$FamilyID <- factor(combi$FamilyID)
```

Break apart the dataset into train and test again so we can run predictions.
```{r}
train <- combi[1:891,]
test <- combi[892:1309,]

fit <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Title + FamilySize + FamilyID,
             data = train,
             method = "class")
fancyRpartPlot(fit)
# create a submission
Prediction <- predict(fit, test, type = "class")
submit <- data.frame(PassengerID = test$PassengerId, Survived = Prediction)
write.csv(submit, file = "withengineeredfeatures.csv", row.names = F)

```

# prediction 7b: any other features we can think of

some tickets have text prepending them, including PC, CA, WC, WEP, SOTON/OQ.  What do these mean?
```{r 7b}
combi$Ticket_type <- sapply(combi$Ticket, FUN = function(x) {
  if (grepl("[A-Z]", x)) {
    # strip away characters
    x <- substr(x,1,regexpr(" [^ ]*$",x)-1)
    # remove spaces and periods
    x <- gsub('[ .]','',x)
    x
  }
  else {
    "Other"
  }
  })
combi$Ticket_type <- factor(combi$Ticket_type)
table(combi$Ticket_type)
table(combi$FamilyID)
```

I'll also give a dummy that says whether the ticket had a cabin assigned or not, and what deck the cabin was on.
```{r}
combi$has_cabin <- 0
combi$has_cabin[grepl('[A-Z]',combi$Cabin)] <- 1

# cabin deck
combi$cabin_level <- NA
combi$cabin_level[combi$has_cabin == 1] <- substr(combi$Cabin[combi$has_cabin == 1], 1, 1)
combi$cabin_level <- factor(combi$cabin_level)
```


Break apart the dataset into train and test again so we can run predictions.
```{r}
train <- combi[1:891,]
test <- combi[892:1309,]

# removing familyID
fit <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Title + FamilySize  + Ticket_type + cabin_level,
             data = train,
             method = "class",
             control = rpart.control(minsplit = 10))
fancyRpartPlot(fit)

# create a submission
Prediction <- predict(fit, test, type = "class")
submit <- data.frame(PassengerID = test$PassengerId, Survived = Prediction)
write.csv(submit, file = "withengineeredfeatures2.csv", row.names = F)

```


# prediction 8 using random forests

To use random forests, we'll need to impute missing values.  Run prediction 7 first.
```{r 8}
# impute age
Agefit <- rpart(Age ~ Pclass + Sex + SibSp + Parch + Fare + Embarked + Title + FamilySize,
                data = combi[!is.na(combi$Age),],
                method = "anova")
combi$Age[is.na(combi$Age)] <- predict(Agefit, combi[is.na(combi$Age),])

# assign missing emarked
combi$Embarked[c(62,830)] = "S"
combi$Embarked <- factor(combi$Embarked)

# impute missing fare
combi$Fare[1044] <- median(combi$Fare, na.rm = T)

# reduce levels in FamilyID
combi$FamilyID2 <- combi$FamilyID
combi$FamilyID2 <- as.character(combi$FamilyID2)
combi$FamilyID2[combi$FamilySize <= 3] <- 'Small'
combi$FamilyID2 <- factor(combi$FamilyID2)
```

Having cleaned the data up, let's run the model

```{r}
# randomness is introduced in this model, so set a seed for reproducible results
combi$Sex <- factor(combi$Sex)
train <- combi[1:891,]
test <- combi[892:1309,]

set.seed(215)
fit <- randomForest(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare +
                      Embarked + Title + FamilySize + FamilyID2,
                    data = train,
                    importance = TRUE,
                    ntree = 200)
varImpPlot(fit)

Prediction <- predict(fit, test)
submit <- data.frame(PassengerID = test$PassengerId, Survived = Prediction)
write.csv(submit, file = "firstforest.csv", row.names = F)
```

# prediciton 9 using conditional inference trees
This ensemble can handle more levels of factors than random forests can.

```{r 9}
set.seed(215)
fit <- cforest(as.factor(Survived) ~  Pclass + Sex + Age + SibSp + Parch + Fare +
                      Embarked + Title + FamilySize + FamilyID,
                    data = train,
                    controls = cforest_unbiased(ntree = 200, mtry = 3))
Prediction <- predict(fit, test, OOB = TRUE, type = "response")
submit <- data.frame(PassengerID = test$PassengerId, Survived = Prediction)
write.csv(submit, file = "conditionalforest.csv", row.names = F)
```


